{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Importando as bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "01a154ff-9996-43e4-8849-032335722555",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#libs\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, regexp_replace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9808b804-c0c0-4108-9f89-a5127fded708",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Criando a pasta que ira receber os arquivos parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eda7823e-06c3-4483-b324-abc5602ef775",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "create_bdfs = dbutils.fs.mkdirs(\"/dbfs/FileStore/data\")\n",
    "if create_bdfs:\n",
    "    print(\"DBFS Created\")\n",
    "else:\n",
    "    print(\"DBFS creation Failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Codigo para os items 3 e 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PostgresqlToParquet:\n",
    "    def __init__(self, host: str, database_name: str, username: str, password: str, table_list: list) -> None:\n",
    "        self.db_params = {\n",
    "            \"url\": f\"jdbc:postgresql://{host}/{database_name}\",\n",
    "            \"user\": username,\n",
    "            \"password\": password,\n",
    "            \"driver\": \"org.postgresql.Driver\"\n",
    "        }\n",
    "        self.spark = SparkSession.builder.appName(\"Items_3_and_4\").getOrCreate()\n",
    "        self.table_list = table_list\n",
    "\n",
    "    def sql_to_parquet(self) -> None:\n",
    "        try:\n",
    "            for table_name in self.table_list:\n",
    "                query = f\"(SELECT * FROM {table_name}) AS subquery\"\n",
    "                df = self.spark.read.jdbc(url=self.db_params[\"url\"], table=query, properties=self.db_params)\n",
    "                parquet_filename = f\"/FileStore/data/{table_name}.parquet\"\n",
    "                df.write.parquet(parquet_filename)\n",
    "                print(f\"{parquet_filename} Saved!!!\")\n",
    "        except:\n",
    "            print(\"Parquet file already saved\")\n",
    "            pass\n",
    "\n",
    "    def load_postgresql_table(self):\n",
    "        postgresql_dfs = {}\n",
    "        for table_name in self.table_list:\n",
    "            query = f\"(SELECT * FROM {table_name}) AS subquery\"\n",
    "            df = self.spark.read.jdbc(url=self.db_params[\"url\"], table=query, properties=self.db_params)\n",
    "            postgresql_dfs[table_name] = df\n",
    "\n",
    "        return postgresql_dfs\n",
    "\n",
    "    def merge_postgresql_to_parquet(self):\n",
    "        # Load PostgreSQL tables into Spark session\n",
    "        postgresql_dfs = self.load_postgresql_table()\n",
    "\n",
    "        for table_name in self.table_list:\n",
    "            parquet_path = f\"/FileStore/data/{table_name}.parquet\"\n",
    "\n",
    "            # Read Parquet file into a DataFrame\n",
    "            parquet_df = self.spark.read.parquet(parquet_path)\n",
    "            parquet_df.createOrReplaceTempView(\"parquet_table\")\n",
    "\n",
    "            # Get the corresponding PostgreSQL DataFrame\n",
    "            postgresql_df = postgresql_dfs[table_name]\n",
    "            postgresql_df.createOrReplaceTempView(\"postgresql_table\")\n",
    "\n",
    "            # Merge logic using Spark SQL\n",
    "            merge_sql = f\"\"\"\n",
    "                MERGE INTO parquet_table AS target\n",
    "                USING postgresql_table AS source\n",
    "                ON target.check_number = source.check_number\n",
    "                WHEN MATCHED THEN\n",
    "                    UPDATE SET\n",
    "                        target.customer_number = source.customer_number,\n",
    "                        target.check_number = source.check_number,\n",
    "                        target.payment_date = source.payment_date,\n",
    "                        target.amount = source.amount\n",
    "                WHEN NOT MATCHED BY target THEN\n",
    "                    INSERT *\n",
    "                WHEN NOT MATCHED BY source THEN\n",
    "                    DELETE\n",
    "                \"\"\"\n",
    "\n",
    "            self.spark.sql(merge_sql)\n",
    "            self.spark.catalog.dropTempView(\"postgresql_table\")\n",
    "            print(f\"Merge operation for {table_name} Completed!!!\")\n",
    "            self.spark.stop()\n",
    "\n",
    "table_list=[\n",
    "    \"payments\",\n",
    "    \"customers\",\n",
    "    \"employees\",\n",
    "    \"offices\",\n",
    "    \"orderdetails\",\n",
    "    \"orders\",\n",
    "    \"product_lines\",\n",
    "    \"products\",\n",
    "]\n",
    "\n",
    "database = PostgresqlToParquet(\n",
    "    database_name=\"ecom1692111647951egphgoeocortbeiy\",\n",
    "    username=\"deumjhxhzvgcoaunnugtbenu@psql-mock-database-cloud\",\n",
    "    password=\"tcinixmdjluazpkqptdhnxrh\",\n",
    "    host=\"psql-mock-database-cloud.postgres.database.azure.com\",\n",
    "    table_list=table_list\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e1cc848a-6332-42b8-8a2a-4d6e7702209a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 3 - Extraindo cada tabela do PostgreSQL e salvando como parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46e23368-2545-483b-bef2-eadb2bf8f147",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "database.sql_to_parquet()\n",
    "\n",
    "# Observando os arquivos parquet baixados\n",
    "display(dbutils.fs.ls(\"dbfs:/FileStore/data\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4 - Fazendo o Merge entre as tabelas postgreSQL e os arquivos parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database.merge_postgresql_to_parquet()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f5832be-c54e-4fdc-9d9b-3bc667a9dab5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 5 - Lendo os arquivos parquet para a analise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8712bb5a-a695-4d9f-89a0-8e17aee78e8d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Spark Session\n",
    "spark = SparkSession.builder.appName(\"Item_5\").getOrCreate()\n",
    "\n",
    "# Parquet para Dataframes\n",
    "customers_df = spark.read.parquet(\"dbfs:/FileStore/data/customers.parquet\")\n",
    "orders_df = spark.read.parquet(\"dbfs:/FileStore/data/orders.parquet\")\n",
    "orderdetails_df = spark.read.parquet(\"dbfs:/FileStore/data/orderdetails.parquet\")\n",
    "products_df = spark.read.parquet(\"dbfs:/FileStore/data/products.parquet\")\n",
    "payments_df = spark.read.parquet(\"dbfs:/FileStore/data/payments.parquet\")\n",
    "employees_df = spark.read.parquet(\"dbfs:/FileStore/data/employees.parquet\")\n",
    "offices_df = spark.read.parquet(\"dbfs:/FileStore/data/offices.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bd53d9c9-fd5c-491e-adfa-735bca55a25f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### 5.1 - Qual país possui a maior quantidade de itens cancelados?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f2aa0bc4-3c65-4493-b79f-cc80c9e74f41",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Analise\n",
    "cancelled_orders_df = customers_df.join(orders_df, \"customer_number\") \\\n",
    "    .filter(col(\"status\") == \"Cancelled\") \\\n",
    "    .groupBy(\"country\") \\\n",
    "    .count() \\\n",
    "    .orderBy(col(\"count\").desc())\n",
    "\n",
    "cancelled_orders_df.show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5533d5db-7f36-4ff6-9fca-3868608000d9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### 5.2 - Qual o faturamento da linha de produto mais vendido, considere como os itens Shipped, cujo o pedido foi realizado no ano de 2005?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "652f2271-312c-4958-92f8-9523479c468a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Usando os filtros necessarios na tabela orders\n",
    "shipped_2005_df = orders_df.filter((col(\"status\") == \"Shipped\") & (col(\"order_date\").between(\"2005-01-01\", \"2006-01-01\")))\n",
    "\n",
    "# Analise\n",
    "product_sales_df = shipped_2005_df.join(orderdetails_df, \"order_number\") \\\n",
    "    .join(products_df, \"product_code\") \\\n",
    "    .join(payments_df, \"customer_number\") \\\n",
    "    .groupBy(\"product_name\") \\\n",
    "    .agg({\"amount\": \"sum\", \"order_number\": \"count\"}) \\\n",
    "    .withColumnRenamed(\"count(order_number)\", \"sales_quantity\") \\\n",
    "    .withColumnRenamed(\"sum(amount)\", \"billing\") \\\n",
    "    .orderBy(col(\"sales_quantity\").desc())\n",
    "\n",
    "product_sales_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c6bb2229-cc5d-49e9-8b99-1e92d0894d7f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### 5.3 - Nome, sobrenome e e-mail dos vendedores do Japão, o local-part do e-mail deve estar mascarado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7651f402-bbc3-4344-a336-c7f641abf4bc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Analise\n",
    "japan_sellers = employees_df.join(offices_df, employees_df.office_code == offices_df.office_code, \"left\") \\\n",
    "    .filter(offices_df.country == \"Japan\") \\\n",
    "    .select(\n",
    "        employees_df.first_name,\n",
    "        employees_df.last_name,\n",
    "        regexp_replace(employees_df.email, \".*@\", \"*****@\").alias(\"masked_email\"),  # Extract username part of email\n",
    "        offices_df.country\n",
    "    )\n",
    "\n",
    "japan_sellers.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3a683549-da8f-4668-9244-7cbfacc5c816",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 6 - Salvando os resultados em tabelas com o formato delta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8fee09fd-2600-4502-b283-2936f27c3c71",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cancelled_orders_df.write.format(\"delta\").mode(\"overwrite\").save(\"/dbfs/FileStore/table/cancelled_orders_result.delta\")\n",
    "product_sales_df.write.format(\"delta\").mode(\"overwrite\").save(\"/dbfs/FileStore/table/product_sales_result.delta\")\n",
    "japan_sellers.write.format(\"delta\").mode(\"overwrite\").save(\"/dbfs/FileStore/table/japan_sellers.delta\")\n",
    "\n",
    "# Observando as tabelas salvas\n",
    "display(dbutils.fs.ls(\"/dbfs/FileStore/table\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Conferindo se os dados estao corretos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ea2e3ea-3faa-439c-9711-793bbc319175",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Lendo as tabelas\n",
    "cancelled_orders_df = spark.read.format(\"delta\").load(\"/dbfs/FileStore/table/cancelled_orders_result.delta\")\n",
    "product_sales_df = spark.read.format(\"delta\").load(\"/dbfs/FileStore/table/product_sales_result.delta\")\n",
    "japan_sellers_df = spark.read.format(\"delta\").load(\"/dbfs/FileStore/table/japan_sellers.delta\")\n",
    "\n",
    "cancelled_orders_df.show(5)\n",
    "product_sales_df.show(5)\n",
    "japan_sellers_df.show(5)\n",
    "\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "main",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
